# üö® Database Common Issues & Pitfalls

## ‚ö†Ô∏è CRITICAL - ALWAYS CHECK BEFORE ADDING MODELS

This file documents all issues encountered during database development to prevent repetition.

---

## üî• **RECURRING PATTERNS (CRITICAL - READ FIRST!)**

### ‚ö†Ô∏è Pattern #1: **Incomplete Schema Mapping** (Issues #14, #20, #21)
**Severity:** CRITICAL  
**Frequency:** 3 times in 1 day  
**Impact:** Data loss, broken functionality, empty permissions, no UI menu

#### The Problem:
When creating database models or migration scripts, fields are often **missed** or **not fully mapped** between:
1. **Pydantic Models** (`core/security/models.py`) ‚Üê Business logic models
2. **SQLAlchemy Models** (`database/models/*.py`) ‚Üê Database schema
3. **JSON Source Data** (`data/security/*.json`) ‚Üê Legacy data
4. **Migration Scripts** (`scripts/migrate_to_db_complete.py`) ‚Üê Data transfer logic

#### Examples:
- **Issue #14:** `User` SQLAlchemy model missing 11 fields that exist in Pydantic model
- **Issue #20:** `Role` SQLAlchemy model missing 4 fields that exist in Pydantic model
- **Issue #21:** `migrate_roles()` function not storing `permissions` field (even though it exists in both models!)
- **Issue #22, #23, #24 (THE REAL ROOT CAUSE):** Migration script was **generating new UUIDs** instead of using JSON IDs directly, causing all ID mismatch issues

#### Why It Keeps Happening:
- **No Automated Validation:** No tool to compare schemas across layers
- **Manual Field Mapping:** Copy-paste errors, forgetting fields
- **Incremental Development:** Adding fields to one layer, forgetting others
- **Lack of Cross-Reference:** Not checking Pydantic model when writing SQLAlchemy model

#### The Solution (MANDATORY):
1. **Before creating ANY new model:**
   ```bash
   # 1. Read the Pydantic model first
   cat core/security/models.py | grep "class YourModel"
   
   # 2. List ALL fields in the Pydantic model
   # Write them down in a checklist
   
   # 3. Create SQLAlchemy model with ALL fields
   # Check off each field as you add it
   
   # 4. Write migration script with ALL fields
   # Check off each field as you map it
   ```

2. **Create a Schema Comparison Script:** (URGENT TODO!)
   ```python
   # scripts/compare_schemas.py
   # Compares Pydantic vs SQLAlchemy vs JSON vs Migration
   # Flags any missing fields
   ```

3. **Add to Pre-Commit Hook:**
   ```bash
   # Run schema comparison before every commit
   python scripts/compare_schemas.py
   ```

4. **Document Field Mapping:**
   ```python
   # In migration script, add comments:
   role = Role(
       # Pydantic: id ‚Üí SQLAlchemy: id
       id=role_uuid,
       # Pydantic: name ‚Üí SQLAlchemy: name
       name=role_data['name'],
       # Pydantic: permissions ‚Üí SQLAlchemy: permissions ‚úÖ
       permissions=json.dumps(role_data.get('permissions', [])),
       # ... document EVERY field mapping
   )
   ```

#### Impact If Ignored:
- **Data Loss:** Fields not migrated = permanent data loss
- **Broken Business Logic:** Missing `permissions` = no RBAC = security breach
- **Silent Failures:** Code runs but features don't work (e.g., empty menu)
- **Hours of Debugging:** Chasing "why isn't X working?" when the issue is a missing field

---

### ‚ö†Ô∏è Pattern #2: **Reserved SQLAlchemy Attribute Names** (Issue #1)
**Severity:** CRITICAL  
**Frequency:** 2 times  
**Impact:** `InvalidRequestError`, application crashes

#### The Problem:
Using `metadata` as a column name (or other reserved attributes like `registry`, `__tablename__`, etc.)

#### Prevention:
```bash
# Before adding ANY column, check:
grep -r "reserved" .cursorrules
grep -r "metadata = Column" database/models/
```

---

### ‚ö†Ô∏è Pattern #3: **PostgreSQL-Specific Types** (Issues #6, #8, #9)
**Severity:** HIGH  
**Frequency:** 3 times  
**Impact:** Platform lock-in, deployment failures on non-PostgreSQL databases

#### The Problem:
Using `UUID(as_uuid=True)`, `JSONB`, `ARRAY`, `INET`, native `ENUM` types.

#### Prevention:
```bash
# Check before committing:
grep -r "from sqlalchemy.dialects.postgresql" database/models/
# Should return: nothing!
```

---

## üõ°Ô∏è **ZERO-TOLERANCE CHECKLIST (Before ANY Database Work)**

Use this checklist EVERY TIME you work on database code:

### üìã Pre-Work Checklist:
- [ ] Read `database/COMMON_ISSUES.md` (this file!)
- [ ] Review **Recurring Patterns** section above
- [ ] Run `./scripts/comprehensive_db_check.sh`
- [ ] Check `.cursorrules` for latest rules

### üìã During-Work Checklist (for new models):
- [ ] List ALL fields from Pydantic model (write them down!)
- [ ] Create SQLAlchemy model with ALL fields (check them off!)
- [ ] Write migration with ALL fields (check them off!)
- [ ] Use database-agnostic types (from `database/types.py`)
- [ ] Avoid reserved attribute names (`metadata`, `registry`)
- [ ] Document field mappings in migration comments

### üìã Pre-Commit Checklist:
- [ ] Run `./scripts/comprehensive_db_check.sh` (automated via pre-commit hook)
- [ ] Verify: `python -c "from database.models import *"` (no errors)
- [ ] Review changes against COMMON_ISSUES.md (check for repeated mistakes)
- [ ] Test migration locally (2+ times, with rollback)

---

## üî¥ **RESERVED WORDS IN SQLALCHEMY**

### Issue #1: `metadata` is Reserved
**Date:** 2026-01-12  
**Severity:** CRITICAL  
**Occurrences:** 2 times (user.py, then all new models)

#### Problem:
```python
# ‚ùå WRONG - SQLAlchemy reserves 'metadata'
class Agent(Base):
    metadata = Column(JSONB)
```

#### Error:
```
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' 
is reserved when using the Declarative API.
```

#### Solution:
```python
# ‚úÖ CORRECT - Use alternative name
class Agent(Base):
    extra_metadata = Column(JSONB)
    # OR
    user_metadata = Column(JSONB)
    # OR
    custom_data = Column(JSONB)
```

#### Why:
SQLAlchemy uses `metadata` internally for table/schema management. It's part of `Base.metadata`.

#### Prevention:
**ALWAYS search existing models for reserved words before creating new ones!**

---

## üî¥ **FOREIGN KEY ISSUES**

### Issue #2: Circular Dependencies in Foreign Keys
**Date:** 2026-01-11  
**Severity:** HIGH

#### Problem:
Foreign keys between tables can cause circular dependency errors during table creation.

#### Error:
```
Foreign key associated with column 'users.org_id' could not find 
table 'organizations' with which to generate a foreign key
```

#### Solutions:
1. **Temporary:** Remove ForeignKey constraints, keep column as UUID
```python
# ‚úÖ Works for now
org_id = Column(UUID(as_uuid=True), index=True)
```

2. **With use_alter:**
```python
# ‚úÖ Deferred constraint creation
org_id = Column(UUID(as_uuid=True), 
                ForeignKey('organizations.id', use_alter=True, 
                          name='fk_user_organization'))
```

3. **Future:** Use Alembic migrations with explicit order

---

## üü° **RELATIONSHIP DEFINITIONS WITHOUT FOREIGN KEYS**

### Issue #3: Relationships Require ForeignKeys
**Date:** 2026-01-11  
**Severity:** MEDIUM

#### Problem:
```python
# ‚ùå WRONG - relationship() needs ForeignKey
org_id = Column(UUID)  # No ForeignKey
organization = relationship("Organization", back_populates="users")
```

#### Error:
```
NoForeignKeysError: Could not determine join condition between 
parent/child tables on relationship User.sessions
```

#### Solution:
Either:
1. Add ForeignKey to column
2. OR remove relationship() definition
3. OR specify explicit primaryjoin

**Current Strategy:** Remove both FKs and relationships temporarily.

---

## üü° **PYDANTIC WARNINGS**

### Issue #4: Protected Namespace Warnings
**Date:** Ongoing  
**Severity:** LOW (warning only)

#### Warning:
```
UserWarning: Field "model_id" has conflict with protected namespace "model_"
```

#### Solution:
Add to model config:
```python
from pydantic import BaseModel, ConfigDict

class MyModel(BaseModel):
    model_config = ConfigDict(protected_namespaces=())
    model_id: str  # Now allowed
```

**Status:** Not blocking, can be addressed later.

---

## üî¥ **DATA MIGRATION ISSUES**

### Issue #5: Duplicate Key Violations on Unique Constraints
**Date:** 2026-01-12  
**Severity:** HIGH  
**Occurrences:** 1 time (organization slug during migration)

#### Problem:
Migration script checked only for existing `id` (primary key) but not for other unique constraints like `slug`, `email`, etc.

```python
# ‚ùå WRONG - Only checks primary key
existing = session.query(Organization).filter_by(id=org_uuid).first()
if not existing:
    session.add(org)  # Can fail if slug already exists!
```

#### Error:
```
psycopg2.errors.UniqueViolation: duplicate key value violates 
unique constraint "ix_organizations_slug"
DETAIL: Key (slug)=(default) already exists.
```

#### Root Cause:
- First migration: Creates org with `id=uuid1`, `slug='default'`
- Second migration: New UUID generated `id=uuid2`, but `slug='default'` still exists
- Database rejects because `slug` has unique constraint

#### Solution:
```python
# ‚úÖ CORRECT - Check ALL unique constraints
existing = session.query(Organization).filter(
    (Organization.id == org_uuid) | 
    (Organization.slug == slug)
).first()

if existing:
    # UPDATE existing record
    existing.name = org_data['name']
    existing.settings = org_data['settings']
    existing.updated_at = datetime.utcnow()
    print(f"‚è≠Ô∏è  Organization '{name}' already exists (updating)")
else:
    # CREATE new record
    org = Organization(...)
    session.add(org)
    print(f"‚úÖ Migrated organization: {name}")

session.commit()
```

#### Best Practices for Migration Scripts:

1. **Check ALL unique constraints, not just primary key:**
   ```python
   # For Organizations:
   - id (primary key)
   - slug (unique)
   
   # For Users:
   - id (primary key)
   - email (unique)
   
   # For Roles:
   - id (primary key)
   - (org_id, name) if org-scoped
   ```

2. **Use UPDATE instead of failing:**
   ```python
   # ‚úÖ IDEMPOTENT - Can run multiple times safely
   if existing:
       update_record(existing, new_data)
   else:
       create_record(new_data)
   ```

3. **Log clearly:**
   ```python
   if existing:
       print(f"‚è≠Ô∏è  Record already exists (updating)")
   else:
       print(f"‚úÖ Created new record")
   ```

4. **Test migration script multiple times:**
   ```bash
   # Should be safe to run repeatedly
   python scripts/migrate_to_db.py  # Run 1
   python scripts/migrate_to_db.py  # Run 2 - should update, not fail
   ```

#### Why This Matters:
- **Railway deployments:** May run migration on every deploy
- **Rollbacks:** Old deployment with migration can re-run
- **Development:** Developers run migrations multiple times locally
- **Data fixes:** Need to re-run migration after fixing source data

#### Prevention:
- ‚úÖ Always identify ALL unique constraints in target table
- ‚úÖ Use filter with OR conditions for all unique fields
- ‚úÖ Implement UPSERT pattern (update if exists, insert if not)
- ‚úÖ Test migration scripts at least 2 times consecutively
- ‚úÖ Use database constraints as checklist for migration logic

---

## üî¥ **ENUM TYPE MISMATCHES**

### Issue #6: Invalid Enum Value During Migration
**Date:** 2026-01-12  
**Severity:** HIGH  
**Occurrences:** 1 time (tool type during migration)

#### Problem:
JSON data contains enum values that don't exist in the database Enum definition.

```python
# ‚ùå JSON has: type = "website"
# ‚ùå Database Enum: API, DATABASE, RAG, EMAIL, WEB_SCRAPING, WEB_SEARCH...
# ‚ùå No "WEBSITE" in enum!
```

#### Error:
```
psycopg2.errors.InvalidTextRepresentation: invalid input value 
for enum tooltype: "website"
LINE 1: ...'::UUID, 'website',...
```

#### Root Cause:
- **Mismatch between legacy data and schema:**
  - Old JSON files use `type = "website"`
  - New database schema didn't include `"website"` in ToolType enum
  - PostgreSQL strictly validates enum values

#### Solution - Two Approaches:

**1. Add Missing Enum Values (Preferred):**
```python
# ‚úÖ Update Enum to include all legacy values
class ToolType(str, Enum):
    API = "api"
    DATABASE = "database"
    RAG = "rag"
    EMAIL = "email"
    WEB_SCRAPING = "web_scraping"
    WEB_SEARCH = "web_search"
    WEBSITE = "website"  # ‚Üê Add missing value
    SLACK = "slack"
    # ... etc
```

**2. Map Legacy Values in Migration Script:**
```python
# ‚úÖ Normalize/map legacy values during migration
TYPE_MAPPING = {
    'website': 'website',      # Direct mapping
    'web': 'web_scraping',     # Legacy ‚Üí New
    'scraper': 'web_scraping',
    'scraping': 'web_scraping',
    'search': 'web_search',
}

tool_type = tool_data.get('type', 'api').lower()
if tool_type in TYPE_MAPPING:
    tool_type = TYPE_MAPPING[tool_type]
elif tool_type not in VALID_TYPES:
    print(f"‚ö†Ô∏è  Unknown tool type '{tool_type}', using 'custom'")
    tool_type = 'custom'
```

#### Best Practices for Enums:

1. **Include 'CUSTOM' or 'OTHER' catch-all:**
   ```python
   class ToolType(str, Enum):
       # ... other types ...
       CUSTOM = "custom"  # Catch-all for unknown types
   ```

2. **Document enum values in migration:**
   ```python
   # At top of migration script
   VALID_TOOL_TYPES = ['api', 'database', 'rag', 'email', ...]
   ```

3. **Validate before migrating:**
   ```python
   # Pre-migration validation
   for tool in tools:
       if tool['type'] not in VALID_TOOL_TYPES:
           print(f"WARNING: Invalid type '{tool['type']}'")
   ```

4. **Use TYPE_MAPPING for backward compatibility:**
   - Old system uses different type names
   - New system standardizes enum values
   - Migration script bridges the gap

5. **PostgreSQL Enum Alterations:**
   ```sql
   -- Adding new enum value to existing enum type
   ALTER TYPE tooltype ADD VALUE 'website';
   -- Note: This requires Alembic migration, not CREATE TABLE
   ```

#### Why This Matters:
- **Data integrity:** PostgreSQL enforces enum constraints strictly
- **Migration safety:** Legacy data must map to new schema
- **Type evolution:** Systems evolve, enums grow over time
- **Multi-source data:** Different sources may use different terminology

#### Prevention:
- ‚úÖ Audit existing JSON data for all unique enum values BEFORE creating models
- ‚úÖ Include "CUSTOM" or "OTHER" as catch-all in all enums
- ‚úÖ Create TYPE_MAPPING dictionary in migration scripts
- ‚úÖ Log warnings for unknown values (don't fail silently)
- ‚úÖ Use Alembic for enum alterations in production

#### ‚ö†Ô∏è **IMPORTANT: Enterprise-Grade Solution Applied**

The initial quick fix (adding 'WEBSITE' to model, hard-coded TYPE_MAPPING) was **NOT enterprise-ready**.

**‚úÖ Proper Enterprise Solution Implemented:**

1. **Centralized Enum Management** (`database/enums.py`):
   ```python
   # Single source of truth for all enums
   class ToolType(str, Enum):
       API = "api"
       DATABASE = "database"
       # ... all types
       CUSTOM = "custom"
       
       @classmethod
       def from_legacy(cls, value: str) -> 'ToolType':
           """Convert legacy value, fallback to CUSTOM"""
           # Handles normalization and mapping
   
   # Centralized legacy mapping
   TOOL_TYPE_LEGACY_MAPPING = {
       'web': ToolType.WEB_SCRAPING,
       'website': ToolType.WEBSITE,
       # ... all legacy mappings
   }
   ```

2. **Alembic Migrations** (Not direct model changes):
   ```bash
   # Proper way to alter enums in production
   alembic init alembic
   alembic revision -m "add_website_to_tooltype"
   alembic upgrade head
   ```

3. **Models Import from Centralized Enums**:
   ```python
   # database/models/tool.py
   from ..enums import ToolType  # Not local definition!
   ```

4. **Migration Script Uses Enum Methods**:
   ```python
   # Use centralized logic
   tool_type_enum = ToolType.from_legacy(legacy_value)
   # Not hard-coded TYPE_MAPPING in migration script
   ```

**Why This Matters:**
- ‚úÖ **Single Source of Truth:** One place to manage all enums
- ‚úÖ **Reusability:** API validators, migrations, models all use same enums
- ‚úÖ **Type Safety:** Centralized validation logic
- ‚úÖ **Maintainability:** Add new type once, works everywhere
- ‚úÖ **Schema Versioning:** Alembic tracks all enum changes
- ‚úÖ **Rollback Capability:** Can revert enum changes via Alembic

**Enterprise Best Practices:**
- üè¢ Never modify models directly in production
- üè¢ Always use migration tools (Alembic)
- üè¢ Centralize business logic (enums, validators)
- üè¢ Document all schema changes
- üè¢ Test migrations before production
- üè¢ Provide rollback paths

#### ‚ö†Ô∏è **CRITICAL: PostgreSQL Enum Types Are Separate from Python Enums!**

**Issue #6 Continued - The REAL Problem:**

Even after adding `WEBSITE` to Python enum and using centralized `database/enums.py`, migration still failed with:

```
invalid input value for enum tooltype: "WEBSITE"
'type': 'WEBSITE'
```

**Root Cause:**
- ‚úÖ Python enum has `WEBSITE` (code level)
- ‚ùå PostgreSQL `tooltype` enum does NOT have `'website'` (database level)
- üî• **Python enums ‚â† PostgreSQL enum types!**

**Why:**
1. When `CREATE TABLE tools` first ran, PostgreSQL created `tooltype` enum with only original values
2. Adding to Python enum (`database/enums.py`) only affects Python code
3. PostgreSQL enum type is **separate** and must be altered explicitly
4. ALTER TYPE requires migration, not just model changes

**Enterprise Solution Applied:**

**1. Alembic Migration Created:**
```python
# alembic/versions/001_add_website_tooltype.py
def upgrade():
    # Check if 'website' exists in PostgreSQL enum
    # If not, add it via ALTER TYPE
    op.execute("COMMIT")  # Must be outside transaction
    op.execute("ALTER TYPE tooltype ADD VALUE 'website'")
```

**2. Automatic Migration Execution:**
```python
# database/init_db.py
def main():
    init_db()  # Create tables
    
    # Run Alembic migrations
    from alembic.config import Config
    from alembic import command
    alembic_cfg = Config("alembic.ini")
    command.upgrade(alembic_cfg, "head")
```

**3. Dockerfile Integration:**
```bash
# Already calls init_db.py which now runs migrations
python database/init_db.py
```

**Key Learnings:**
- üìö PostgreSQL enum types exist independently of Python enums
- üìö Changing Python code does NOT change database schema
- üìö Must use ALTER TYPE or DROP/CREATE to modify PostgreSQL enums
- üìö Alembic migrations are the proper enterprise way
- üìö First migration must add missing enum value
- üìö Future enum additions follow same pattern

**Prevention:**
- ‚úÖ Never assume Python enum changes affect database
- ‚úÖ Always create Alembic migration for enum modifications
- ‚úÖ Test migrations in dev environment before production
- ‚úÖ Document all schema changes in migration files
- ‚úÖ Verify PostgreSQL enum values match Python enum values

---

## üî¥ **IMPORT ERRORS AFTER TYPE CONVERSION**

### Issue #7: NameError After Database-Agnostic Conversion
**Date:** 2026-01-12  
**Severity:** HIGH  
**Occurrences:** 1 time (after conversion script)

#### Problem:
After converting from PostgreSQL-specific types to database-agnostic types, models had import errors.

```python
# ‚ùå WRONG Import Pattern:
from ..types import UUID, JSON as JSONB

# Code uses:
user_metadata = Column(JSON, default={})
# Error: NameError: name 'JSON' is not defined
```

#### Error:
```
NameError: name 'JSON' is not defined. Did you mean: 'JSONB'?
```

#### Root Cause:
- Conversion script changed `Column(JSONB, ...)` to `Column(JSON, ...)`
- BUT import was `JSON as JSONB` (only JSONB available in scope)
- Code now references `JSON` which doesn't exist
- Inconsistent naming between import and usage

#### Solution:
```python
# ‚úÖ CORRECT Import Pattern:
from ..types import UUID, JSON, JSONArray
JSONB = JSON  # Alias for backward compatibility

# Now both work:
user_metadata = Column(JSON, default={})  # ‚úÖ
config = Column(JSONB, default={})        # ‚úÖ
```

#### Best Practices for Type Conversions:

1. **Import Both Names:**
   ```python
   from ..types import UUID, JSON, JSONArray
   JSONB = JSON  # Keep alias for compatibility
   ```

2. **Update Conversion Script:**
   ```python
   # Pattern for replacements:
   REPLACEMENTS = [
       # Import: Add all needed types
       (r'from sqlalchemy\.dialects\.postgresql import UUID, JSONB',
        'from ..types import UUID, JSON, JSONArray\nJSONB = JSON'),
       
       # Usage: Update column definitions
       (r'Column\(JSONB', 'Column(JSON'),
   ]
   ```

3. **Test Imports Before Commit:**
   ```bash
   # Quick validation
   python -c "from database.models import User, Agent, Tool"
   # Should not raise ImportError
   ```

4. **Consistent Naming Strategy:**
   - Choose: `JSON` (new standard) OR `JSONB` (PostgreSQL legacy)
   - Use consistently across all models
   - Keep alias for transition period

5. **Automated Validation:**
   ```bash
   # Check for undefined names in models
   grep -r "Column(JSON" database/models/*.py
   # Ensure JSON is imported in all files
   ```

#### Why This Matters:
- **Deployment Failures:** ImportError crashes application startup
- **Silent Errors:** May not be caught until runtime
- **Inconsistency:** Mix of JSON/JSONB confuses developers
- **Migration Risk:** Type conversion scripts must be tested

#### Prevention:
- ‚úÖ Always import all types used in model
- ‚úÖ Test imports immediately after conversion
- ‚úÖ Keep alias during transition (JSONB = JSON)
- ‚úÖ Run `python -c "from database.models import *"` before commit
- ‚úÖ Update conversion script to handle imports correctly
- ‚úÖ Document which name to use (JSON preferred)

---

## üî¥ **INET TYPE - PostgreSQL-Specific**

### Issue #8: INET Column Type (PostgreSQL-Specific)
**Date:** 2026-01-12  
**Severity:** HIGH  
**Occurrences:** 1 time (found in audit.py)

#### Problem:
Using PostgreSQL's `INET` type for IP addresses breaks database agnosticism.

```python
# ‚ùå WRONG - PostgreSQL-specific:
from sqlalchemy.dialects.postgresql import INET
ip_address = Column(INET, nullable=False)
```

#### Error:
```
NameError: name 'INET' is not defined
```

Or on other databases:
```
sqlalchemy.exc.CompileError: INET type is not supported on this database
```

#### Root Cause:
- `INET` is PostgreSQL-specific for IPv4/IPv6 addresses
- Not available in MySQL, SQLite, SQL Server, Oracle
- Breaks multi-database compatibility

#### Solution:
```python
# ‚úÖ CORRECT - Database-agnostic:
ip_address = Column(String(45), nullable=False)  # IPv4/IPv6
```

**Why String(45)?**
- IPv4 max: `255.255.255.255` (15 chars)
- IPv6 max: `ffff:ffff:ffff:ffff:ffff:ffff:255.255.255.255` (45 chars)
- Works on ALL databases
- Application-level validation if needed

#### Fixed Files:
```bash
‚úÖ database/models/audit.py (3 occurrences)
   - AuditLog.ip_address
   - SecurityEvent.ip_address
   - DataExport.ip_address
```

#### Why This Matters:
- **Enterprise Requirement:** Platform must work with any database
- **Customer Choice:** Let customers use their preferred database
- **Migration Freedom:** Easy to switch databases
- **No Vendor Lock-in:** Not tied to PostgreSQL

#### Prevention:
```bash
# Check before committing:
grep -r "INET" database/models/
grep -r "from sqlalchemy.dialects.postgresql" database/models/

# Or run comprehensive check:
./scripts/comprehensive_db_check.sh
```

**Rule:** For IP addresses, ALWAYS use `String(45)` to support both IPv4 and IPv6 on all databases.

---

## üî¥ **MODEL NAME CONFLICTS - Database vs Core**

### Issue #10: Import Name Conflict Between DB and Core Models
**Date:** 2026-01-12  
**Severity:** HIGH  
**Occurrences:** 1 time (UserMFA in user_service.py)

#### Problem:
Same model name exists in both `database.models` (SQLAlchemy) and `core.security.models` (Pydantic), causing import conflicts.

```python
# ‚ùå WRONG - Ambiguous import:
from database.models.user import UserMFA  # SQLAlchemy Table
from core.security import UserMFA  # Pydantic Model
# Both named "UserMFA" - conflict!
```

#### Error:
```
ImportError: cannot import name 'UserMFA' from 'database.models.user'
```

#### Root Cause:
- **Database layer**: `UserMFA` is SQLAlchemy Table (DB schema)
- **Core layer**: `UserMFA` is Pydantic Model (API contract)
- **Service layer** needs both but names conflict
- Python imports last definition, overwriting first

#### Solution Pattern:
```python
# ‚úÖ CORRECT - Use aliases for DB models:
from ..models.user import User as DBUser
from ..models.user import UserMFA as UserMFATable  # DB table
from core.security import User, UserMFA  # API models

# Clear distinction:
db_user = DBUser(...)  # Database
core_user = User(...)  # API
```

#### Convention Established:
```python
# ALWAYS use this pattern in services:

# 1. Import DB models with descriptive alias
from ..models.user import User as DBUser
from ..models.user import UserSession as DBSession
from ..models.user import UserMFA as UserMFATable

# 2. Import Core models without prefix
from core.security import User, Session, UserMFA

# 3. Conversion function clearly named
def _db_to_core_user(db_user: DBUser) -> User:
    pass
```

#### Why This Matters:
- **Type Safety:** IDE distinguishes models
- **Clarity:** Code readers understand layer
- **Maintainability:** Changes don't break other layer

#### Prevention:
```bash
# Check for ambiguous imports:
grep -r "from database.models.* import" database/services/ | grep -v "as DB"
```

**Rule:** Database models ALWAYS imported with alias. Core models imported without prefix.

---

## üî¥ **POSTGRESQL ENUM TYPE PERSISTS**

### Issue #9: Native PostgreSQL Enum in Database Despite Model Fix
**Date:** 2026-01-12  
**Severity:** CRITICAL  
**Occurrences:** 1 time (tools table)

#### Problem:
Even after changing model from `SQLEnum` to `String(50)`, the database table STILL has the old PostgreSQL enum type.

```python
# ‚úÖ Model is CORRECT:
type = Column(String(50), nullable=False)

# ‚ùå But Database has:
CREATE TABLE tools (
    type tooltype NOT NULL  -- ‚Üê PostgreSQL enum still exists!
);
```

#### Error:
```
sqlalchemy.exc.DataError: invalid input value for enum tooltype: "website"
```

#### Root Cause:
- **Changing Python code does NOT change existing database schema**
- PostgreSQL enum type `tooltype` was created in the past
- SQLAlchemy doesn't automatically drop old enum types
- Table must be dropped and recreated to remove enum

#### Solution:

**Option 1: Drop & Recreate Table (‚úÖ Used)**
```python
# scripts/fix_tools_table.py
conn.execute(text("DROP TABLE IF EXISTS tools CASCADE"))
conn.execute(text("DROP TYPE IF EXISTS tooltype CASCADE"))
Tool.__table__.create(engine)  # Recreates with VARCHAR
```

**Option 2: Alembic Migration (Enterprise)**
```python
# alembic/versions/002_remove_tool_enum.py
def upgrade():
    # Create new column
    op.add_column('tools', sa.Column('type_new', sa.String(50)))
    # Copy data
    op.execute("UPDATE tools SET type_new = type::text")
    # Drop old column and enum
    op.drop_column('tools', 'type')
    op.execute("DROP TYPE IF EXISTS tooltype")
    # Rename column
    op.alter_column('tools', 'type_new', new_column_name='type')

def downgrade():
    # Reverse process
    pass
```

#### Why This Happened:
1. Originally used `SQLEnum(ToolType, native_enum=True)`
2. PostgreSQL created `CREATE TYPE tooltype AS ENUM (...)`
3. Changed model to `String(50)` (correct!)
4. BUT: Existing database schema didn't change
5. Migration script tried to insert 'website' into enum (not allowed)

#### Fixed Files:
```bash
‚úÖ database/models/tool.py (already correct - String(50))
‚úÖ scripts/fix_tools_table.py (NEW - drops/recreates table)
‚úÖ Dockerfile (runs fix script before migration)
```

#### Prevention:
```bash
# NEVER use native PostgreSQL enums:
‚ùå type = Column(SQLEnum(ToolType, native_enum=True, ...))

# ALWAYS use String + Python enum validation:
‚úÖ type = Column(String(50), nullable=False)
‚úÖ # Validate with: ToolType(value)  # raises ValueError if invalid
```

**Key Learning:** 
- **Code changes ‚â† Database changes**
- Always use migrations for schema changes
- SQLAlchemy doesn't auto-drop unused enum types
- Native database enums are not database-agnostic

#### Deployment Strategy:
```bash
1. Database init creates all tables
2. fix_tools_table.py drops/recreates tools table
3. Migration script inserts data
4. Result: VARCHAR column, works on all databases
```

---

## üîµ **BEST PRACTICES LEARNED**

### 1. Column Naming Conventions
```python
# ‚úÖ GOOD
extra_metadata = Column(JSONB)
user_metadata = Column(JSONB)
custom_data = Column(JSONB)
additional_info = Column(JSONB)

# ‚ùå AVOID
metadata = Column(JSONB)      # Reserved by SQLAlchemy
type = Column(String)         # Python builtin
id = Column(String)           # Use UUID or Integer
```

### 2. Reserved Words to NEVER Use
- `metadata` - SQLAlchemy internal
- `registry` - SQLAlchemy internal  
- `__tablename__` - Already used by SQLAlchemy
- `__mapper__` - SQLAlchemy internal
- `__table__` - SQLAlchemy internal

### 3. Import Order Matters
When using ForeignKeys, import models in dependency order:
```python
# ‚úÖ CORRECT ORDER
from .organization import Organization  # No dependencies
from .role import Role                  # Depends on Organization
from .user import User                  # Depends on Organization + Role
```

### 4. Index Naming
```python
# ‚úÖ GOOD - Explicit names
Index('idx_user_org_email', User.org_id, User.email)

# ‚ùå AVOID - Auto-generated names can clash
Index(User.org_id, User.email)  # No explicit name
```

---

## üìã **PRE-COMMIT CHECKLIST**

Before committing any database model changes:

- [ ] Search for `metadata = Column` in all models
- [ ] Search for Python reserved words (type, id as simple String, etc.)
- [ ] Check all ForeignKey definitions have `use_alter=True` if circular
- [ ] Verify import order (dependencies first)
- [ ] Run `python database/init_db.py` locally
- [ ] Check for Pydantic namespace conflicts
- [ ] Ensure all timestamps use `datetime.utcnow` not `datetime.now()`
- [ ] Verify all UUID columns use `UUID(as_uuid=True)`
- [ ] Check index names are explicit and unique
- [ ] **Test imports:** `python -c "from database.models import *"` ‚Üê NEW!
- [ ] **Check database-agnostic types:** No PostgreSQL-specific imports
- [ ] **Verify JSON imports:** If `Column(JSON` exists, `JSON` must be imported

**Before committing migration scripts:**

- [ ] Identify ALL unique constraints in target tables (not just primary keys)
- [ ] Use UPSERT pattern (check existence, update if exists, insert if not)
- [ ] Check with OR conditions for all unique fields: `(Model.id == x) | (Model.slug == y)`
- [ ] Test migration script at least 2 times consecutively (must be idempotent)
- [ ] Verify all relationship mappings (e.g., org_id, role_ids)
- [ ] Handle both UUID and non-UUID legacy IDs with smart mapping
- [ ] Log clear messages for create vs. update vs. skip
- [ ] Include error handling with rollback on each record
- [ ] Add verification step at end to count migrated records

**Before committing type conversion scripts:**

- [ ] Test resulting imports: `python -c "from database.models import User, Agent, Tool"`
- [ ] Verify both old and new type names work (e.g., JSON and JSONB)
- [ ] Check all Column definitions use correct type names
- [ ] Ensure no duplicate imports (e.g., `UUID, UUID`)
- [ ] Run validation script: `./scripts/validate_db_models.sh`

---

## üîç **QUICK SEARCH COMMANDS**

Before adding new models, run these searches:

```bash
# Check for 'metadata' usage
grep -r "metadata = Column" database/models/

# Check for common reserved words
grep -r "type = Column" database/models/
grep -r "id = Column(String" database/models/

# Check for relationship definitions
grep -r "relationship(" database/models/

# Check for ForeignKey usage
grep -r "ForeignKey(" database/models/
```

---

## üìö **REFERENCES**

- [SQLAlchemy Reserved Attributes](https://docs.sqlalchemy.org/en/14/orm/mapping_api.html#sqlalchemy.orm.registry)
- [SQLAlchemy use_alter for FKs](https://docs.sqlalchemy.org/en/14/core/constraints.html#use-alter)
- [Pydantic Protected Namespaces](https://docs.pydantic.dev/latest/concepts/models/#protected-namespaces)

---

## üî¥ **PYDANTIC TYPE MISMATCH - DB TO CORE CONVERSION**

### Issue #17: Validation Errors - Type Conversion Missing
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (database/services/user_service.py)

#### Problem:
When loading users from database, got 5 Pydantic validation errors due to type mismatches between Database model (SQLAlchemy) and Core model (Pydantic).

```python
# ‚ùå WRONG - Direct assignment without type conversion
return User(
    id=db_user.id,  # UUID object, expects str
    org_id=db_user.org_id,  # UUID object, expects str
    auth_provider=None,  # None, but field is REQUIRED (no Optional)
    role_ids=db_user.role_ids,  # String '[]', expects List[str]
    group_ids=db_user.group_ids,  # String '[]', expects List[str]
)
```

#### Error:
```
pydantic_core._pydantic_core.ValidationError: 5 validation errors for User
id
  Input should be a valid string [type=string_type, input_value=UUID('cbcc4ec3-...'), input_type=UUID]
org_id
  Input should be a valid string [type=string_type, input_value=UUID('2c969bf1-...'), input_type=UUID]
auth_provider
  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]
role_ids
  Input should be a valid list [type=list_type, input_value='[]', input_type=str]
group_ids
  Input should be a valid list [type=list_type, input_value='[]', input_type=str]
```

#### Root Cause:
**Database Model** returns Python native types:
- `id`, `org_id`, `department_id`: `UUID` objects
- `role_ids`, `group_ids`: JSON strings like `'[]'` or `'["uuid1","uuid2"]'`
- `auth_provider`: `None` (for existing users without this field set)

**Core Pydantic Model** expects:
- `id: str` (line 365 in core/security/models.py)
- `org_id: str` (line 366)
- `auth_provider: AuthProvider` - **REQUIRED field!** (line 371)
- `role_ids: List[str]` (line 383)
- `group_ids: List[str]` (line 382)

No type conversion was performed in `_db_to_core_user` mapping function.

#### Solution:
```python
# ‚úÖ CORRECT - Type conversions for Pydantic validation
import json

# 1. Convert UUID to string
user_id = str(db_user.id) if db_user.id else str(uuid.uuid4())
org_id = str(db_user.org_id) if db_user.org_id else ""
department_id = str(db_user.department_id) if db_user.department_id else None

# 2. Convert auth_provider to enum (required field!)
try:
    auth_provider = AuthProvider(db_user.auth_provider) if db_user.auth_provider else AuthProvider.LOCAL
except (ValueError, AttributeError):
    auth_provider = AuthProvider.LOCAL  # Default to LOCAL

# 3. Parse JSON strings to lists
try:
    role_ids = json.loads(db_user.role_ids) if isinstance(db_user.role_ids, str) else (db_user.role_ids or [])
except (json.JSONDecodeError, TypeError):
    role_ids = []

try:
    group_ids = json.loads(db_user.group_ids) if isinstance(db_user.group_ids, str) else (db_user.group_ids or [])
except (json.JSONDecodeError, TypeError):
    group_ids = []

return User(
    id=user_id,  # ‚úÖ str
    org_id=org_id,  # ‚úÖ str
    auth_provider=auth_provider,  # ‚úÖ AuthProvider enum
    role_ids=role_ids,  # ‚úÖ List[str]
    group_ids=group_ids,  # ‚úÖ List[str]
    # ... rest of fields
)
```

#### Why:
1. **UUID ‚Üí str**: Pydantic expects string IDs, not UUID objects
2. **JSON string ‚Üí List**: Database stores arrays as JSON strings (database-agnostic), must parse
3. **None ‚Üí Enum**: `auth_provider` is a **required field** with no `Optional`, must have default

#### Prevention:
- ‚úÖ **ALWAYS check Pydantic field types before mapping:**
  ```bash
  # Read the Core model first
  grep "class User" -A 50 core/security/models.py | grep "auth_provider\|role_ids"
  ```
- ‚úÖ **ALWAYS convert database types:**
  - UUID objects ‚Üí `str(uuid_obj)`
  - JSON strings ‚Üí `json.loads(json_str)`
  - Nullable enums ‚Üí provide default value
- ‚úÖ **Test with actual database data:**
  ```python
  db_user = session.query(DBUser).first()
  print(f"id type: {type(db_user.id)}")  # <class 'uuid.UUID'>
  print(f"role_ids: {repr(db_user.role_ids)}")  # '[]'
  ```
- ‚úÖ **Add type hints to catch mismatches:**
  ```python
  def _db_to_core_user(db_user: DBUser) -> User:
      # IDE will warn if types don't match
  ```

#### Related Issues:
- Issue #12: MFA Schema Mismatch (similar root cause)
- Issue #14: Missing Database Columns (schema sync)
- Issue #15: Schema Update Without Migration

---

## üî¥ **MISSING IMPORTS AFTER CODE CHANGES**

### Issue #18: `NameError` - Used Type Without Importing
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (database/services/user_service.py)

#### Problem:
After adding type conversion for `auth_provider` field (Issue #17), used `AuthProvider` enum but forgot to add it to the import statement.

```python
# ‚ùå WRONG - Import statement missing AuthProvider
from core.security import User, Session as CoreSession, UserStatus, MFAMethod

# ... later in code ...
auth_provider = AuthProvider(db_user.auth_provider)  # ‚Üê NameError!
                ^^^^^^^^^^^^
```

#### Error:
```
NameError: name 'AuthProvider' is not defined
  File "/app/database/services/user_service.py", line 184, in _db_to_core_user
    auth_provider = AuthProvider(db_user.auth_provider) if db_user.auth_provider else AuthProvider.LOCAL
                    ^^^^^^^^^^^^
```

#### Root Cause:
When fixing Issue #17 (type conversion), added code that uses `AuthProvider` enum, but didn't update the import statement to include it.

**Common scenario:**
1. Fix a bug by adding new code
2. New code uses a type/class
3. Forget to import the new type
4. Code fails at runtime (not caught by syntax check)

#### Solution:
```python
# ‚úÖ CORRECT - Include all used types in import
from core.security import User, Session as CoreSession, UserStatus, MFAMethod, AuthProvider, UserProfile, UserMFA
```

#### Why:
Python doesn't check imports until runtime. The code compiles successfully, but fails when the line is executed.

#### Prevention:
- ‚úÖ **ALWAYS check imports after adding new code:**
  ```bash
  # List all used types in the file
  grep -o "[A-Z][a-zA-Z]*(" database/services/user_service.py | sort -u
  
  # Compare with imports
  grep "from core.security import" database/services/user_service.py
  ```
- ‚úÖ **Use IDE type checking** (e.g., PyCharm, VSCode with Pylance)
- ‚úÖ **Test import before committing:**
  ```python
  python3 -c "from database.services.user_service import UserService; print('‚úÖ OK')"
  ```
- ‚úÖ **Add to pre-commit hook:**
  ```bash
  # Check for undefined names
  python3 -m pyflakes database/services/*.py
  ```

#### Related Issues:
- Issue #17: Type conversion (root cause of needing AuthProvider)
- Issue #11: PBKDF2 import error (similar import issue)

---

## üî¥ **DATA MIGRATION INCOMPLETE - MISSING FIELDS**

### Issue #19: Missing `role_ids` in User Migration
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (scripts/migrate_to_db_complete.py)

#### Problem:
Users were migrated from JSON to database without their `role_ids`, causing:
- Empty permissions list in frontend
- `hasPermission()` always returns false
- Menu items hidden for all users (even Super Admin)

```python
# ‚ùå WRONG - Migration script missing role_ids
user = User(
    id=user_uuid,
    email=user_data['email'],
    # ... other fields ...
    # ‚ùå NO role_ids field!
)
```

#### Error (User-facing):
```
Frontend: Empty menu
Console: hasPermission() returns false
Backend: user.role_ids = [] (empty)
Login Response: "permissions": [] (empty list)
```

#### Root Cause:
The migration script (`migrate_to_db_complete.py`) was missing the code to:
1. Extract `role_ids` from JSON user data
2. Map old role IDs to new UUIDs (using `role_mapping`)
3. Store `role_ids` in database as JSON string

**Why this happened:**
When creating the migration script, focused on basic user fields (email, password, profile) but forgot the RBAC-related fields (`role_ids`, `department_id`, `group_ids`).

#### Solution:

**Part 1: Quick Fix - Update Existing Users**

Created `scripts/update_user_role_ids.py`:
```python
# Load role_ids from JSON files
user_roles = load_users_from_json()

# Update existing database users
with Session() as session:
    db_users = session.query(DBUser).all()
    
    for db_user in db_users:
        if email in user_roles:
            json_role_ids = user_roles[email]
            db_user.role_ids = json.dumps(json_role_ids)
    
    session.commit()
```

**Part 2: Fix Migration Script for Future**

Updated `scripts/migrate_to_db_complete.py`:
```python
# ‚úÖ CORRECT - Map role_ids from JSON
role_ids_json = []
for old_role_id in user_data.get('role_ids', []):
    if old_role_id in role_mapping:
        role_ids_json.append(str(role_mapping[old_role_id]))
    else:
        # Try UUID as-is
        try:
            uuid.UUID(old_role_id)
            role_ids_json.append(old_role_id)
        except ValueError:
            print(f"‚ö†Ô∏è  Unknown role_id '{old_role_id}', skipping")

user = User(
    # ... other fields ...
    role_ids=json.dumps(role_ids_json),  # ‚úÖ Store as JSON string
)
```

**Part 3: Integrate into Deployment**

Updated `Dockerfile`:
```bash
python scripts/migrate_to_db_complete.py
python scripts/update_user_role_ids.py  # ‚úÖ Run after migration
```

#### Why:
1. **Quick Fix:** Updates existing users immediately without re-migration
2. **Prevention:** Fixed migration script for new deployments
3. **Idempotent:** Update script safe to run multiple times
4. **No Data Loss:** Only updates users that exist in both JSON and DB

#### Prevention:
- ‚úÖ **ALWAYS review Pydantic model fields before migration:**
  ```bash
  # Compare DB model with Core model
  grep "class User" -A 100 core/security/models.py | grep "role_ids\|department_id\|group_ids"
  grep "class User" -A 100 database/models/user.py | grep "role_ids\|department_id\|group_ids"
  ```
- ‚úÖ **Test migration with real data:**
  ```bash
  # After migration, verify fields
  python -c "from database.services import UserService; u = UserService.get_all_users()[0]; print(f'role_ids: {u.role_ids}')"
  ```
- ‚úÖ **Check frontend after deployment:**
  - Login as user
  - Verify menu items appear
  - Check browser console for permission errors
- ‚úÖ **Add to migration checklist:**
  - [ ] Basic fields (email, password)
  - [ ] Profile fields (name, phone)
  - [ ] **RBAC fields (role_ids, department_id, group_ids)** ‚Üê Don't forget!
  - [ ] Security fields (MFA, status)
  - [ ] Timestamps (created_at, updated_at)

#### Related Issues:
- Issue #17: Type conversion (UUID, JSON parsing)
- Issue #14: Missing database columns
- Issue #15: Schema update without migration

---

## üî¥ **MISSING DATABASE COLUMNS IN ROLE MODEL**

### Issue #20: `Role` model missing critical columns (permissions, parent_id, level, created_by)
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (during role loading from database)

#### Problem:
The `database.models.role.Role` SQLAlchemy model was missing several critical columns that exist in the `core.security.Role` Pydantic model:
- `permissions` (List[str]): Array of permission strings
- `parent_id` (Optional[UUID]): For role hierarchy/inheritance
- `level` (int): Privilege level (0 = super admin, 100 = default)
- `created_by` (Optional[str]): User who created the role

When `RoleService._db_to_core_role` tried to map these fields, it resulted in `AttributeError: 'Role' object has no attribute 'permissions'`.

#### Error:
```
üìä Attempting to load roles from database...
‚ùå Database roles error: AttributeError: 'Role' object has no attribute 'permissions'
üìÇ Loading roles from files (database unavailable)
```

This caused the system to fall back to loading roles from JSON files, meaning:
- Users loaded from the database had UUID-based `role_ids`
- Roles loaded from files had string-based IDs
- `security_state.roles` dictionary was keyed by string IDs, not UUIDs
- `PolicyEngine` couldn't find roles for users ‚Üí No permissions ‚Üí No UI menu

#### Root Cause:
Incomplete schema definition during initial database model creation. The SQLAlchemy `Role` model (`database/models/role.py`) was a simplified version that only included basic fields (`id`, `name`, `description`, `is_system`, `org_id`, timestamps), missing the critical fields required for RBAC functionality.

#### Solution:
1. **Updated `database/models/role.py`:** Added missing columns:
   ```python
   class Role(Base):
       # ... existing fields ...
       
       # Permissions (stored as JSON array of permission strings)
       permissions = Column(JSONArray, default=list)
       
       # Hierarchy
       parent_id = Column(UUID, nullable=True)  # For role inheritance
       level = Column(String(10), default="100")  # Lower = more privileged
       
       # Metadata
       created_by = Column(String(100), nullable=True)
   ```

2. **Created `scripts/add_role_columns.py`:** Idempotent script to add missing columns to existing `roles` table:
   ```python
   columns_to_add = [
       ("permissions", "TEXT", "''"),  # JSON array stored as TEXT
       ("parent_id", "UUID", "NULL"),
       ("level", "VARCHAR(10)", "'100'"),
       ("created_by", "VARCHAR(100)", "NULL"),
   ]
   ```

3. **Created `database/services/role_service.py`:** Service layer for role database operations:
   ```python
   class RoleService:
       @staticmethod
       def get_all_roles() -> List[Role]:
           # Fetch from DB and convert to core.security.Role
           
       @staticmethod
       def _db_to_core_role(db_role: DBRole) -> Role:
           # Parse permissions JSON, handle type conversions
   ```

4. **Updated `Dockerfile`:** Added `scripts/add_role_columns.py` to deployment pipeline.

5. **Verified `core/security/state.py`:** Already imports and uses `RoleService` for dual-read.

#### Why:
Ensures that roles loaded from the database have all the necessary fields to support:
- **RBAC**: Permissions list for policy evaluation
- **Role Hierarchy**: Parent-child role inheritance
- **Privilege Levels**: Fine-grained access control
- **Audit Trail**: Tracking who created roles

This is critical for the UI menu to appear, as the `PolicyEngine` uses `role.permissions` to determine what features a user can access.

#### Prevention:
- ‚úÖ **Schema Comparison Tool:** Before creating any database model, compare the Pydantic model schema (`core/security/models.py`) with the SQLAlchemy model schema to ensure 100% field parity.
- ‚úÖ **Automated Schema Sync Check:** Add a check to `comprehensive_db_check.sh` that compares SQLAlchemy models with their corresponding Pydantic models and flags missing fields.
- ‚úÖ **Migration Test Suite:** Create integration tests that:
  1. Migrate data to DB
  2. Load data from DB into Pydantic models
  3. Verify all fields are correctly populated
  4. Verify business logic (e.g., permissions, menu) works
- ‚úÖ **Documentation:** Update `MASTER_DOCUMENTATION_UPDATED.md` with the complete database schema, including all fields for each table.

---

## üî¥ **MISSING ROLE PERMISSIONS IN MIGRATION**

### Issue #21: Role migration not storing `permissions` field ‚Üí Empty user permissions ‚Üí No UI menu
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (during role migration)

#### Problem:
The `migrate_to_db_complete.py` script's `migrate_roles()` function was creating `Role` objects in the database **without storing the `permissions` field**, even though the field exists in both the SQLAlchemy model and the JSON source data.

```python
# ‚ùå WRONG (scripts/migrate_to_db_complete.py, lines 166-173 before fix):
role = Role(
    id=role_uuid,
    name=role_data['name'],
    description=role_data.get('description', ''),
    is_system=role_data.get('is_system', False),
    org_id=org_uuid,
    # ‚ùå Missing: permissions, parent_id, level, created_by
    created_at=...
)
```

This resulted in **all roles in the database having empty `permissions = []`**, which cascaded into a critical chain of failures.

#### Error Chain:
```
1. Role migrated with permissions = [] (empty)
   ‚Üì
2. User migration: role_ids = ["role_super_admin"] (old string IDs)
   ‚Üì
3. update_user_role_ids.py: Tries to map "role_super_admin" to UUID
   ‚Üì
4. Mapping fails (name mismatch: "role_super_admin" ‚â† "Super Admin")
   ‚Üì
5. User ends up with role_ids = [] (empty!)
   ‚Üì
6. User logs in ‚Üí PolicyEngine._get_user_permissions(user)
   ‚Üì
7. for role_id in user.role_ids:  ‚Üê Nothing happens! (empty list)
   ‚Üì
8. permissions = set()  ‚Üê Stays empty!
   ‚Üì
9. /api/security/auth/me returns: "permissions": []
   ‚Üì
10. UI checks hasPermission("agents:view") ‚Üí returns false
   ‚Üì
11. All menu items hidden ‚Üí NO MENU!
```

#### Symptoms:
- Users loaded from database successfully
- Roles loaded from database successfully (114 roles shown in logs)
- Login works
- **But after login, the UI menu does not appear**
- `/api/security/auth/me` returns `"permissions": []`
- `user.role_ids = []` in the database
- Logs show: `‚úÖ 'admin@agentforge.app' already has correct role_ids: []` ‚Üê **This is NOT correct!**

#### Root Cause:
1. **Missing fields in migration:** The `Role` object constructor in `migrate_to_db_complete.py` did not include `permissions`, `parent_id`, `level`, or `created_by` fields.
2. **Weak role ID mapping:** The `update_user_role_ids.py` script had a brittle mapping strategy that couldn't handle the `"role_super_admin"` (JSON) ‚Üí `"Super Admin"` (DB name) conversion.

#### Solution:

**1. Fixed `scripts/migrate_to_db_complete.py` (lines 166-173):**
```python
# ‚úÖ CORRECT - Store ALL role fields:
role = Role(
    id=role_uuid,
    name=role_data['name'],
    description=role_data.get('description', ''),
    permissions=json.dumps(role_data.get('permissions', [])),  # ‚úÖ Store permissions!
    parent_id=parent_uuid,  # ‚úÖ Store parent_id
    level=str(role_data.get('level', 100)),  # ‚úÖ Store level
    is_system=role_data.get('is_system', False),
    org_id=org_uuid,
    created_by=role_data.get('created_by'),  # ‚úÖ Store created_by
    created_at=datetime.fromisoformat(role_data['created_at']) if 'created_at' in role_data else datetime.utcnow()
)
```

**2. Enhanced `scripts/update_user_role_ids.py`:**
```python
# ‚úÖ CORRECT - Multi-strategy mapping:
# Legacy string ID mapping (e.g., "role_super_admin" ‚Üí "Super Admin")
legacy_name_mapping = {
    "role_super_admin": "Super Admin",
    "role_admin": "Admin",
    "role_manager": "Manager",
    "role_user": "User",
    "role_viewer": "Viewer"
}

# Strategy 1: Check if it's already a UUID (from previous migration)
if old_role_id_str in role_id_to_uuid_map:
    found_uuid = role_id_to_uuid_map[old_role_id_str]

# Strategy 2: Use legacy mapping (e.g., "role_super_admin" ‚Üí "Super Admin")
elif old_role_id_str in legacy_name_mapping:
    role_name = legacy_name_mapping[old_role_id_str]
    if role_name in role_name_to_uuid_map:
        found_uuid = role_name_to_uuid_map[role_name]

# Strategy 3: Direct name match (if somehow the name was stored)
elif old_role_id_str in role_name_to_uuid_map:
    found_uuid = role_name_to_uuid_map[old_role_id_str]
```

#### Why:
- **RBAC depends on permissions:** The entire permission system relies on `role.permissions` being populated.
- **UI menu is permission-driven:** Every menu item checks `hasPermission(permission)`, which ultimately queries `user ‚Üí roles ‚Üí permissions`.
- **Cascading failure:** A single missing field in the migration caused a complete UI failure for all users.

#### Prevention:
- ‚úÖ **Complete Field Mapping Checklist:** When writing migration scripts, create a checklist of ALL fields in both the source (JSON) and destination (SQLAlchemy model). Verify each field is explicitly mapped.
- ‚úÖ **Migration Validation Script:** Add a post-migration validation step that:
  1. Queries a sample of migrated records
  2. Checks that critical fields (e.g., `permissions`, `role_ids`) are not empty
  3. Fails loudly if any critical field is missing
- ‚úÖ **End-to-End Integration Test:** After migration, perform a full login ‚Üí check permissions ‚Üí verify menu appears test.
- ‚úÖ **Schema Comparison Tool (Again!):** This is the 3rd time a missing field caused issues (Issue #14: User columns, Issue #20: Role columns, Issue #21: Role permissions). **We need an automated tool that compares Pydantic model ‚Üî SQLAlchemy model ‚Üî JSON data ‚Üî Migration script and flags any discrepancies.**

---

## üî¥ **EMAIL MISMATCH IN USER LOOKUP**

### Issue #22: Using email for user lookup when emails differ between DB and JSON
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (in update_user_role_ids.py)

#### Problem:
The `update_user_role_ids.py` script was using **email** as the lookup key to match users between the database and JSON file. However, the emails in the two sources **did not match**:

- **Database:** `admin@agentforge.app`, `ahmedhamdi81@yahoo.com`
- **JSON:** `admin@agentforge.to`, `ahamdi.hussein@gmail.com`

This resulted in:
```
‚ö†Ô∏è  No role_ids found in JSON for 'admin@agentforge.app', skipping.
‚ö†Ô∏è  No role_ids found in JSON for 'ahmedhamdi81@yahoo.com', skipping.
‚úÖ Updated 0 users successfully!
```

All users ended up with **empty `role_ids = []`**, leading to no permissions and no UI menu.

#### Error Chain:
```
1. Script looks up user by email: "admin@agentforge.app"
   ‚Üì
2. Searches in JSON for email: "admin@agentforge.app"
   ‚Üì
3. JSON only has: "admin@agentforge.to" ‚Üê Email mismatch!
   ‚Üì
4. Lookup fails ‚Üí No role_ids found
   ‚Üì
5. User keeps empty role_ids = []
   ‚Üì
6. No permissions ‚Üí No menu
```

#### Root Cause:
**Using a mutable, user-facing field (email) as the primary key** for data synchronization between systems. Emails can change, be typos, or differ across systems. The **immutable user ID** should have been used instead.

#### Symptoms:
```
‚ö†Ô∏è  No role_ids found in JSON for 'admin@agentforge.app', skipping.
‚ö†Ô∏è  No role_ids found in JSON for 'ahmedhamdi81@yahoo.com', skipping.
‚úÖ Updated 0 users successfully!
```

Despite the script reporting "success", **zero users were actually updated**.

#### Solution:
Changed the lookup strategy from **email-based** to **ID-based**:

```python
# ‚ùå WRONG - Email-based lookup:
def load_users_from_json():
    user_roles = {}
    for user_data in data.values():
        email = user_data.get('email')
        role_ids = user_data.get('role_ids', [])
        if email:
            user_roles[email] = role_ids  # ‚Üê Key by email
    return user_roles

# In main():
for db_user in db_users:
    email = db_user.email
    old_json_role_ids = json_user_roles.get(email, [])  # ‚Üê Lookup by email (fails!)
```

```python
# ‚úÖ CORRECT - ID-based lookup:
def load_users_from_json():
    user_roles = {}
    for user_id, user_data in data.items():  # ‚Üê JSON already keyed by ID!
        role_ids = user_data.get('role_ids', [])
        if user_id and role_ids:
            user_roles[user_id] = role_ids  # ‚Üê Key by ID
    return user_roles

# In main():
for db_user in db_users:
    user_id = str(db_user.id)  # ‚Üê Use database ID
    old_json_role_ids = json_user_roles.get(user_id, [])  # ‚Üê Lookup by ID (works!)
```

#### Why:
- **User IDs are immutable:** Once assigned, they never change.
- **Emails are mutable:** Users can change emails, typos happen, different systems may store different emails.
- **JSON structure already uses IDs:** The `users.json` file is already structured as `{user_id: user_data}`, so using IDs is the natural choice.

#### Prevention:
- ‚úÖ **Always use IDs for data synchronization:** Never use user-facing fields (email, username, name) as primary keys for cross-system data matching.
- ‚úÖ **Validate assumptions:** Check that the lookup keys actually exist in both sources before processing.
- ‚úÖ **Better logging:** Log both the lookup key (ID) and user-facing identifier (email) for easier debugging:
  ```python
  print(f"   Processing user '{user_id}' (email: {db_user.email})")
  ```
- ‚úÖ **Fail loudly on mismatch:** If no users are found, the script should exit with an error, not silently succeed:
  ```python
  if updated_count == 0 and len(db_users) > 0:
      raise ValueError("No users were updated! Check ID/email matching.")
  ```

---

## üî¥ **USER ID MISMATCH BETWEEN DB AND JSON**

### Issue #23: Migration script skips existing users without preserving ID mapping
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (in migrate_to_db_complete.py + update_user_role_ids.py)
**Related to:** RECURRING PATTERN #1 (Incomplete Schema Mapping)

#### Problem:
The `migrate_to_db_complete.py` script checks if a user already exists in the database (by email). If the user exists, it **skips** the user **without** storing an ID mapping between the **JSON user ID** and the **Database user ID**.

Later, `update_user_role_ids.py` tries to look up users by their **JSON user ID** in the database, but:
- **JSON has:** `85900a07-fe70-473b-99d2-795a46862009`
- **Database has:** `cbcc4ec3-eec9-48dd-9c75-4f63d81a51cc` (a completely different UUID!)

Result: **Lookup fails ‚Üí No role_ids found ‚Üí Empty permissions ‚Üí No menu!**

#### Error Chain:
```
1. First migration: User created with NEW UUID (cbcc4ec3-...)
   ‚Üì
2. Second migration: migrate_to_db_complete.py checks email ‚Üí User exists
   ‚Üì
3. Script SKIPs user without storing: json_id (85900a07-...) ‚Üí db_id (cbcc4ec3-...)
   ‚Üì
4. update_user_role_ids.py looks up by JSON ID: "85900a07-..."
   ‚Üì
5. Database only has: "cbcc4ec3-..." ‚Üê ID mismatch!
   ‚Üì
6. Lookup fails ‚Üí User skipped ‚Üí role_ids = []
   ‚Üì
7. No permissions ‚Üí No menu
```

#### Root Cause:
**Skipping existing records during migration without preserving the ID mapping** between source (JSON) and destination (Database). This breaks any subsequent scripts that rely on ID-based lookups.

#### Code Location:
**`scripts/migrate_to_db_complete.py` (Lines 222-225):**
```python
# Check if already exists
existing = session.query(User).filter_by(email=user_data['email']).first()
if existing:
    print(f"‚è≠Ô∏è  User '{user_data['email']}' already exists, skipping")
    continue  # ‚Üê PROBLEM: No ID mapping stored!
```

#### Symptoms:
```
üìÇ Loading user role_ids from JSON...
     Found 0 users with role_ids in JSON  ‚Üê JSON IDs don't match DB IDs!

‚ö†Ô∏è  No role_ids found in JSON for user ID 'cbcc4ec3-...' (email: admin@agentforge.app), skipping.
‚ö†Ô∏è  No role_ids found in JSON for user ID 'ad997c93-...' (email: ahmedhamdi81@yahoo.com), skipping.
‚úÖ Updated 0 users successfully!  ‚Üê Silent failure!
```

#### Solution:
**Option 1: Use email for lookup (IMMEDIATE FIX - What we did):**

Changed `update_user_role_ids.py` to use **email** as the matching key instead of user ID, since emails are the only stable identifier that matches between JSON and DB:

```python
# ‚úÖ CORRECT - Email-based lookup:
def load_users_from_json():
    user_roles = {}
    for user_id, user_data in data.items():
        email = user_data.get('email')
        role_ids = user_data.get('role_ids', [])
        if email and role_ids:
            user_roles[email] = {
                'json_user_id': user_id,  # For logging
                'role_ids': role_ids
            }
    return user_roles

# In main():
for db_user in db_users:
    email = db_user.email
    user_info = json_user_roles.get(email)  # ‚Üê Lookup by email (works!)
```

**Option 2: Store ID mappings in migration (BETTER LONG-TERM FIX):**

The `migrate_to_db_complete.py` should **always** return an ID mapping, even for skipped users:

```python
# ‚úÖ BETTER - Store ID mapping for existing users:
existing = session.query(User).filter_by(email=user_data['email']).first()
if existing:
    print(f"‚è≠Ô∏è  User '{user_data['email']}' already exists, skipping")
    id_mapping[user_data['id']] = existing.id  # ‚Üê Store mapping!
    continue
```

Then return `id_mapping` from `migrate_users()` and pass it to subsequent scripts.

#### Why:
- **IDs are the natural primary key** for database lookups, but only if they're consistent between source and destination.
- When IDs don't match, we need either:
  1. A **mapping table/dict** to translate between them, OR
  2. Use a **stable alternative identifier** (email, username) that matches in both systems.

#### Prevention:
- ‚úÖ **Always preserve ID mappings:** Even when skipping existing records, store the mapping: `source_id ‚Üí destination_id`.
- ‚úÖ **Return mappings from migration functions:** All `migrate_*` functions should return `(count, id_mapping)`.
- ‚úÖ **Log ID mismatches:** Print warnings when JSON ID ‚â† DB ID.
- ‚úÖ **Use stable identifiers:** When IDs differ, use email/username for cross-system lookups.
- ‚úÖ **Fail loudly on empty results:** If no users are updated, raise an error instead of silently succeeding.

---

## üî¥ **GENERATING NEW IDS INSTEAD OF PRESERVING ORIGINALS**

### Issue #24: Migration script generates new UUIDs instead of using JSON IDs (ROOT CAUSE of #22 & #23)
**Date:** 2026-01-12
**Severity:** CRITICAL
**Occurrences:** 1 time (but caused 3 separate issues: #22, #23, #24)
**Related to:** RECURRING PATTERN #1 (Incomplete Schema Mapping)

#### Problem:
The **ACTUAL root cause** of all ID mismatch issues (#22, #23) was finally discovered:

**The migration script was GENERATING new UUIDs** instead of **using the existing UUIDs from JSON**:

```python
# ‚ùå WRONG - Generates new UUIDs:
try:
    user_uuid = uuid.UUID(old_id)
except ValueError:
    user_uuid = uuid.uuid4()  # ‚Üê PROBLEM: New UUID generated!
    print(f"   üîÑ Generated new UUID for user '{email}': {user_uuid}")
```

**Result:**
- **JSON has:** `85900a07-fe70-473b-99d2-795a46862009`
- **Database gets:** `cbcc4ec3-eec9-48dd-9c75-4f63d81a51cc` (completely different!)
- All subsequent lookups fail because IDs don't match!

#### The Cascade of Issues This Caused:
1. **Issue #22:** Thought it was an email mismatch ‚Üí Tried email-based lookup
2. **Issue #23:** Realized IDs don't match ‚Üí Tried ID mapping workaround
3. **Issue #24:** Finally found the real problem ‚Üí **Don't generate new IDs!**

#### Root Cause:
**Violating a fundamental database principle:**

> **"Never regenerate primary keys during migration!"**

Primary keys (IDs) should be **preserved exactly as-is** from source to destination. Generating new IDs breaks:
- Foreign key references
- Audit trails
- External system integrations
- Data consistency

#### Code Location:
**`scripts/migrate_to_db_complete.py` (Lines 228-233, old version):**

```python
# ‚ùå WRONG:
try:
    user_uuid = uuid.UUID(old_id)
except ValueError:
    user_uuid = uuid.uuid4()  # ‚Üê BAD!
```

#### Solution:
**Use JSON IDs directly. If invalid, SKIP the record (don't create a new ID):**

```python
# ‚úÖ CORRECT:
try:
    user_uuid = uuid.UUID(old_id)
except ValueError:
    print(f"   ‚ùå INVALID UUID in JSON: {old_id}")
    continue  # ‚Üê Skip invalid records, don't create new IDs!
```

**Also, check by ID (not email):**

```python
# ‚ùå WRONG - Check by email:
existing = session.query(User).filter_by(email=user_data['email']).first()

# ‚úÖ CORRECT - Check by ID:
existing = session.query(User).filter_by(id=user_uuid).first()
```

#### Why:
- **IDs are immutable:** They should never change across systems.
- **Primary keys are sacred:** Don't regenerate them unless absolutely necessary (e.g., migrating from non-UUID system).
- **Consistency matters:** JSON is the source of truth; preserve its IDs.

#### Impact:
- ‚úÖ **No more ID mismatches!**
- ‚úÖ **No need for ID mapping workarounds!**
- ‚úÖ **No need for email-based lookups!**
- ‚úÖ **All subsequent scripts work correctly!**

#### Prevention:
- ‚úÖ **Never use `uuid.uuid4()` in migration scripts** unless migrating from a non-UUID system.
- ‚úÖ **Always preserve source IDs:** If source has UUIDs, use them as-is.
- ‚úÖ **Check by ID, not alternative fields:** Use `filter_by(id=...)` not `filter_by(email=...)`.
- ‚úÖ **Skip invalid records:** If UUID is invalid, skip the record with a clear error message.
- ‚úÖ **Add to validation script:** Check that migration scripts don't contain `uuid.uuid4()`.

---

## üîÑ **UPDATE LOG**

| Date | Issue | Status | Notes |
|------|-------|--------|-------|
| 2026-01-13 | **DUPLICATE ROLES: Script only updated first one** #28 | ‚úÖ Fixed | Multiple Super Admin roles exist, script used `.first()` ‚Üí Changed to `.all()` |
| 2026-01-13 | **EMPTY PERMISSIONS: Super Admin role** #27 | ‚úÖ Fixed | Roles had empty permissions[] ‚Üí Menu not appearing! Created fix script. |
| 2026-01-13 | **PYDANTIC VALIDATION ERROR: role_ids double-encoded!** #26 | ‚úÖ Fixed | Database contained double-JSON: `'"[\"uuid\"]"'` ‚Üí Parse twice! |
| 2026-01-12 | Generating new IDs instead of preserving #24 | ‚úÖ Fixed | ROOT CAUSE of #22 & #23! Use JSON IDs directly, no uuid4() |
| 2026-01-12 | User ID mismatch (migration skip) #23 | ‚ö†Ô∏è Symptom | Real cause was Issue #24 (ID generation) |
| 2026-01-12 | Email mismatch in user lookup #22 | ‚ö†Ô∏è Symptom | Real cause was Issue #24 (ID generation) |
| 2026-01-12 | Role permissions not migrated #21 | ‚úÖ Fixed | Added permissions, parent_id, level, created_by to migration + improved role_ids mapping |
| 2026-01-12 | Missing columns in Role model #20 | ‚úÖ Fixed | Added permissions, parent_id, level, created_by columns |
| 2026-01-12 | Missing role_ids UUID mapping #19.2 | ‚úÖ Fixed | Script now maps old IDs to UUIDs |
| 2026-01-12 | Missing role_ids in migration #19 | ‚úÖ Fixed | Created update script + fixed migration |
| 2026-01-12 | Missing import #18 | ‚úÖ Fixed | Added AuthProvider to imports |
| 2026-01-12 | Pydantic type mismatch #17 | ‚úÖ Fixed | Added type conversions (UUID‚Üístr, JSON‚ÜíList, None‚ÜíEnum) |
| 2026-01-12 | Syntax error (extra parenthesis) #13 | ‚úÖ Fixed | Removed duplicate `)` in user_service.py |
| 2026-01-12 | MFA schema mismatch #12 | ‚úÖ Fixed | `method`‚Üí`methods`, `secret`‚Üí`totp_secret` |
| 2026-01-12 | PBKDF2 import error #11 | ‚úÖ Fixed | Changed to `PBKDF2HMAC` + backend param |
| 2026-01-12 | **AUTOMATED PREVENTION** | ‚úÖ **ACTIVE** | Pre-commit hook + comprehensive checks |
| 2026-01-12 | Model name conflict (DB vs Core) | ‚úÖ Fixed | Use aliases for DB imports |
| 2026-01-12 | Script import error (engine) | ‚úÖ Fixed | Changed to get_engine() |
| 2026-01-12 | PostgreSQL enum persists in DB | ‚úÖ Fixed | Drop/recreate tools table |
| 2026-01-12 | INET type (PostgreSQL-specific) | ‚úÖ Fixed | Changed to String(45) for IP addresses |
| 2026-01-12 | audit.py syntax error | ‚úÖ Fixed | JSONB = JSON, INET ‚Üí JSONB = JSON |
| 2026-01-12 | ARRAY(Float) not converted | ‚úÖ Fixed | Changed to JSONArray |
| 2026-01-12 | role.py still imports PG types | ‚úÖ Fixed | Updated to use ..types |
| 2026-01-12 | Import error after conversion | ‚úÖ Fixed | Import JSON + JSONB alias AFTER imports |
| 2026-01-12 | PostgreSQL enum not updated | ‚ö†Ô∏è Abandoned | Switched to VARCHAR (industry standard) |
| 2026-01-12 | Enum type mismatch | ‚úÖ Fixed | Centralized enums + VARCHAR instead of PG enum |
| 2026-01-12 | Migration duplicate key | ‚úÖ Fixed | Check all unique constraints, use UPSERT pattern |
| 2026-01-12 | `metadata` reserved | ‚úÖ Fixed | Renamed to `extra_metadata` in 5 models |
| 2026-01-11 | ForeignKey circular deps | ‚ö†Ô∏è Workaround | Removed FKs temporarily |
| 2026-01-11 | Relationships without FKs | ‚ö†Ô∏è Workaround | Removed relationships |

---

## üõ°Ô∏è **AUTOMATED PREVENTION SYSTEM (NEW!)**

### System Overview:
**3-Layer Defense Against Repeated Mistakes:**

1. **Pre-Commit Hook** (`.git/hooks/pre-commit`)
   - Runs automatically before EVERY commit
   - Blocks commits with database issues
   - Cannot be bypassed easily

2. **Comprehensive Check Script** (`scripts/comprehensive_db_check.sh`)
   - Checks all 7 documented issues
   - Validates PostgreSQL-agnostic design
   - Exit code 1 if ANY error found

3. **Enhanced Documentation** (this file + `.cursorrules`)
   - All issues documented with examples
   - Clear prevention strategies
   - Quick reference checklist

### Usage:

**Manual Check (anytime):**
```bash
./scripts/comprehensive_db_check.sh
```

**Auto Check (on commit):**
```bash
git add database/models/user.py
git commit -m "Update user model"
# ‚úÖ Pre-commit hook runs automatically
# ‚ùå Commit blocked if issues found
```

**Check Status:**
```bash
ls -la .git/hooks/pre-commit  # Should be executable
cat .git/hooks/pre-commit      # Verify content
```

### What It Checks:

| Check | Issue # | Description |
|-------|---------|-------------|
| `metadata` column | #1 | Reserved SQLAlchemy word |
| PostgreSQL imports | #7 | `sqlalchemy.dialects.postgresql` forbidden |
| `ARRAY(...)` usage | #7 | Must use `JSONArray` |
| `UUID(as_uuid=True)` | #7 | Must use `UUID` from `..types` |
| Native enums | #6 | Should use `String` + Python enum |
| Import consistency | #7 | Verify JSON/JSONB imports |
| ForeignKeys | #2 | Circular dependency checks |
| UTC timestamps | Best Practice | All times must be UTC |

---

## üî¥ **CRYPTOGRAPHY LIBRARY API CHANGES**

### Issue #11: `PBKDF2` Import Deprecated
**Date:** 2026-01-12
**Severity:** HIGH
**Occurrences:** 1 time (database/services/encryption.py)

#### Problem:
```python
# ‚ùå WRONG - Deprecated in cryptography v43+
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
```

#### Error:
```
ImportError: cannot import name 'PBKDF2' from 'cryptography.hazmat.primitives.kdf.pbkdf2'
```

Result: Database services crash on init ‚Üí Falls back to files

#### Root Cause:
The `cryptography` library changed its API in v43.0.0+:
- Old (deprecated): `PBKDF2`
- New (correct): `PBKDF2HMAC`

#### Solution:
```python
# ‚úÖ CORRECT - Use PBKDF2HMAC
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.backends import default_backend

kdf = PBKDF2HMAC(
    algorithm=hashes.SHA256(),
    length=32,
    salt=salt,
    iterations=100000,
    backend=default_backend()  # ‚Üê Required parameter
)
```

#### Why:
- `PBKDF2HMAC` is the correct class name (HMAC = Hash-based Message Authentication Code)
- Requires explicit `backend` parameter
- More explicit and secure API

#### Prevention:
- ‚úÖ Check library documentation when upgrading dependencies
- ‚úÖ Test encryption/decryption after library updates
- ‚úÖ Add integration tests for core services (EncryptionService, UserService)

---

## üî¥ **SCHEMA MISMATCH - DATABASE VS CORE MODELS**

### Issue #12: MFA Field Name Mismatch
**Date:** 2026-01-12
**Severity:** HIGH
**Occurrences:** 1 time (database/services/user_service.py)

#### Problem:
```python
# ‚ùå WRONG - Field doesn't exist in UserMFA
mfa=UserMFA(
    enabled=True,
    method=MFAMethod.TOTP,  # ‚Üê No 'method' field!
    secret="..."             # ‚Üê No 'secret' field!
)
```

#### Error:
```
AttributeError: NONE
File "user_service.py", line 168: mfa_method = MFAMethod.NONE
```

(Trying to use `MFAMethod.NONE` which doesn't exist in enum)

#### Root Cause:
Schema mismatch between Database model and Core (Pydantic) model:

**Database Model (database/models/user.py):**
```python
mfa_method: Enum (single value)
mfa_secret_encrypted: str
```

**Core Model (core/security/models.py):**
```python
methods: List[MFAMethod]  # ‚Üê PLURAL, LIST!
totp_secret: Optional[str]  # ‚Üê Different name!
```

#### Solution:
```python
# ‚úÖ CORRECT - Match Core model schema
# Build methods list
mfa_methods = []
if db_user.mfa_enabled and db_user.mfa_method:
    mfa_methods = [MFAMethod(db_user.mfa_method)]

mfa=UserMFA(
    enabled=db_user.mfa_enabled,
    methods=mfa_methods,  # ‚úÖ List[MFAMethod]
    totp_secret=db_user.mfa_secret_encrypted,  # ‚úÖ Correct field
    totp_verified=db_user.mfa_enabled,
    backup_codes=[]
)
```

#### Why:
- Core model supports **multiple MFA methods** (TOTP + SMS + Email)
- Database model currently stores **single method** (will migrate later)
- Mapping layer must convert single ‚Üí list
- Field names differ (`mfa_secret_encrypted` ‚Üí `totp_secret`)

#### Prevention:
- ‚úÖ **ALWAYS check Pydantic model schema before mapping**
- ‚úÖ Read `core/security/models.py` to understand field names
- ‚úÖ Use type hints to catch mismatches early
- ‚úÖ Add validation tests for model conversions

```python
# Best Practice: Type hints help catch errors
def _db_to_core_user(db_user: DBUser) -> User:  # ‚Üê Clear types
    # Implementation with proper field mapping
```

---

## üü° **SYNTAX ERRORS - COPY/PASTE MISTAKES**

### Issue #13: Extra Closing Parentheses
**Date:** 2026-01-12
**Severity:** MEDIUM
**Occurrences:** 1 time (database/services/user_service.py)

#### Problem:
```python
# ‚ùå WRONG - Extra closing parentheses
mfa=UserMFA(...),
),  # ‚Üê Extra!
),  # ‚Üê Extra!
auth_provider=None,  # ‚Üê Python confused
```

#### Error:
```
IndentationError: unexpected indent
File "user_service.py", line 197
  auth_provider=None,
```

#### Root Cause:
Copy-paste error when editing code manually. Added closing parenthesis twice instead of once.

#### Solution:
```python
# ‚úÖ CORRECT - One closing paren
mfa=UserMFA(...),
auth_provider=None,  # ‚Üê Now correct
```

#### Why:
The extra `)` closes the `User(...)` constructor prematurely, then Python sees `auth_provider=...` at the wrong indentation level.

#### Prevention:
- ‚úÖ **Use Python syntax checker before committing:**
  ```bash
  python -m py_compile database/services/user_service.py
  ```
- ‚úÖ **Use IDE/editor with syntax highlighting**
- ‚úÖ **Run local tests before pushing:**
  ```bash
  python -c "from database.services import UserService; print('‚úÖ Import OK')"
  ```
- ‚úÖ **Add pre-commit hook for Python syntax validation:**
  ```bash
  # In .git/hooks/pre-commit
  python -m compileall database/ || exit 1
  ```

---

## üî¥ **DOUBLE JSON ENCODING - DATABASE SERIALIZATION ISSUE**

### Issue #26: Pydantic validation error - `role_ids` is string after parsing (double-encoded)
**Date:** 2026-01-13
**Severity:** CRITICAL
**Occurrences:** 1 time (database/services/user_service.py)
**Related to:** RECURRING PATTERN #1 (Incomplete Schema Mapping)

#### Problem:
After successfully fixing the `fix_user_roles.py` script to assign roles to users, Pydantic validation STILL failed with:

```
ValidationError: 1 validation error for User
role_ids
  Input should be a valid list [type=list_type, input_value='["4e7a15db-..."]', input_type=str]
```

**Database value:**  
`'"[\"4e7a15db-753f-4f9f-ae1a-8ad34a7d9790\"]"'` ‚Üê Double-encoded!

**After 1st `json.loads()`:**  
`'["4e7a15db-753f-4f9f-ae1a-8ad34a7d9790"]'` ‚Üê STILL a string!

**Pydantic expects:**  
`["4e7a15db-753f-4f9f-ae1a-8ad34a7d9790"]` ‚Üê A Python list

#### Root Cause:
The `fix_user_roles.py` script does:
```python
role_ids_json = json.dumps([str(super_admin_role.id)])  # ‚Üê 1st encoding
user.role_ids = role_ids_json  # Store in DB
```

But the `role_ids` column in SQLAlchemy is a `TEXT` column, and when SQLAlchemy stores JSON, it **automatically JSON-encodes** it AGAIN! This causes **double encoding**.

So the database actually contains:
```
Database: '"[\"4e7a15db-...\"]"'  (outer quotes + escaped inner quotes)
```

When we `json.loads()` once, we get:
```python
'["4e7a15db-..."]'  # Still a string!
```

We need to `json.loads()` **TWICE** to get the actual list!

#### Error Logs (Debug Output):
```
üîç DEBUG [ahmedhamdi81@yahoo.com] role_ids type: <class 'str'>, value: '"[\"4e7a15db-...\"]"'
üîç DEBUG [ahmedhamdi81@yahoo.com] role_ids is STRING, parsing...
üîç DEBUG [ahmedhamdi81@yahoo.com] Parsed role_ids: ["4e7a15db-..."] (type: <class 'str'>)
‚ùå Error in get_all_users: ValidationError: ... input_value='["4e7a15db-..."]', input_type=str
```

Notice: **After parsing, it's STILL a string, not a list!**

#### Solution:
```python
# ‚úÖ CORRECT - Handle double JSON encoding
if isinstance(db_user.role_ids, str):
    role_ids = json.loads(db_user.role_ids) if db_user.role_ids else []
    
    # üî• CRITICAL: If result is STILL a string, parse AGAIN!
    if isinstance(role_ids, str):
        print(f"   üî• DOUBLE-ENCODED DETECTED! Parsing again...")
        role_ids = json.loads(role_ids)
    
    # Final sanity check
    if not isinstance(role_ids, list):
        print(f"   ‚ö†Ô∏è  Final role_ids is STILL not a list! Defaulting to []")
        role_ids = []
```

**Files Changed:**
- `database/services/user_service.py` (lines 188-230): Added double-encoding detection and second parse

#### Why Double Encoding Happens:
1. **Manual `json.dumps()` in script:** Converts list ‚Üí JSON string
2. **SQLAlchemy auto-serialization:** SQLAlchemy sees it's a TEXT column storing JSON and encodes AGAIN
3. **Result:** Double-encoded JSON in database

**Alternative approaches to prevent this:**
1. **Use SQLAlchemy's JSON type properly:** Let SQLAlchemy handle encoding/decoding automatically
2. **Store raw strings:** Don't pre-encode with `json.dumps()` if SQLAlchemy will do it
3. **Use ARRAY type (if database-agnostic):** Use `JSONArray` custom type from `database/types.py`

#### Impact:
- **User loading failed:** SecurityState couldn't load users from database
- **Fallback to files:** Application fell back to loading from JSON files
- **No database integration:** All database work was bypassed due to this validation error
- **Hours of debugging:** Spent significant time tracking down the double encoding

#### Prevention:
1. ‚úÖ **Always test Pydantic validation after DB writes:**
   ```bash
   # After running fix script
   python -c "from database.services import UserService; UserService.get_all_users()"
   ```

2. ‚úÖ **Add explicit type checking after JSON parsing:**
   ```python
   parsed = json.loads(db_value)
   if isinstance(parsed, str):
       # Handle double encoding!
       parsed = json.loads(parsed)
   assert isinstance(parsed, list), f"Expected list, got {type(parsed)}"
   ```

3. ‚úÖ **Use SQLAlchemy's JSON type correctly:**
   ```python
   # In models:
   role_ids = Column(JSON, default=list)  # Let SQLAlchemy handle encoding
   
   # In scripts:
   user.role_ids = ["uuid1", "uuid2"]  # Pass Python list, not JSON string!
   ```

4. ‚úÖ **Document JSON storage patterns:**
   - If column is `TEXT`: Store raw JSON string, SQLAlchemy won't auto-encode
   - If column is `JSON`: Pass Python objects, SQLAlchemy auto-encodes
   - Never mix both approaches!

5. ‚úÖ **Add to pre-commit checks:**
   ```bash
   # Test that parsing returns expected types
   python scripts/test_db_json_parsing.py
   ```

#### Related Issues:
- Issue #19: Missing `role_ids` in migration (original problem)
- Issue #21: Role permissions not stored (related to data flow)
- RECURRING PATTERN #1: Incomplete schema mapping across layers

---

## üî¥ **EMPTY PERMISSIONS - ROLE DATA NOT FULLY MIGRATED**

### Issue #27: Super Admin role has empty permissions array
**Date:** 2026-01-13
**Severity:** CRITICAL
**Occurrences:** 1 time (discovered during login testing)
**Related to:** RECURRING PATTERN #1 (Incomplete Schema Mapping)

#### Problem:
After successfully fixing all database schema and data migration issues (Issues #1-26), users could log in but **the menu was not appearing**. Investigation revealed:

**API Response (`/api/security/auth/me`):**
```json
{
  "id": "cbcc4ec3-eec9-48dd-9c75-4f63d81a51cc",
  "email": "admin@agentforge.app",
  "role_ids": ["4e7a15db-753f-4f9f-ae1a-8ad34a7d9790"],  ‚Üê ‚úÖ Role assigned
  "roles": [{...}],                                      ‚Üê ‚úÖ Role object present
  "permissions": []                                      ‚Üê ‚ùå EMPTY!
}
```

**Database State:**
```sql
SELECT id, name, permissions FROM roles WHERE name = 'Super Admin';
-- Result:
-- id: 4e7a15db-753f-4f9f-ae1a-8ad34a7d9790
-- name: Super Admin
-- permissions: [] or NULL or ""  ‚Üê EMPTY!
```

#### Root Cause:
1. **Source Data Issue:** The `data/security/roles.json` file either:
   - Had empty `permissions` arrays for roles, OR
   - The `permissions` field was missing entirely

2. **Migration Issue:** The `migrate_roles()` function in `scripts/migrate_to_db_complete.py` correctly **read and stored** the `permissions` field (Issue #21 fixed), but if the source JSON had empty permissions, it faithfully migrated empty data!

3. **No Default Permissions:** When creating roles, no logic existed to populate default permissions for system roles like "Super Admin"

#### Impact:
- **UI Menu Not Appearing:** Frontend checks `user.permissions` array to decide which menu items to show
- **Empty Array = No Menu:** With `permissions: []`, all menu items are hidden
- **Login Works But Useless:** Users can authenticate but cannot access any features
- **Silent Failure:** No error in logs - just empty permissions array

#### Why This Happened:
This is **RECURRING PATTERN #1** again:
1. ‚úÖ Pydantic `User` model has `permissions: List[str]` field
2. ‚úÖ SQLAlchemy `Role` model has `permissions` column
3. ‚úÖ Migration script stores `permissions` field from JSON
4. ‚ùå **BUT:** JSON source data has empty `permissions` arrays!
5. ‚ùå **AND:** No validation to ensure system roles have required permissions
6. ‚ùå **RESULT:** Valid schema, but invalid/empty data migrated

#### Error Symptoms:
```javascript
// Browser console (checking API response):
const user = await fetch('/api/security/auth/me').then(r => r.json());
console.log(user.permissions);  // Output: []  ‚Üê Problem!

// Expected for Super Admin:
console.log(user.permissions);
// Output: ["agents.view", "agents.create", "users.manage", ...]
```

#### Solution:
Created `scripts/fix_super_admin_permissions.py` to populate Super Admin role with all system permissions:

```python
ALL_PERMISSIONS = [
    "agents.view", "agents.create", "agents.edit", "agents.delete",
    "tools.view", "tools.create", "tools.edit", "tools.delete",
    "kb.view", "kb.create", "kb.edit", "kb.delete",
    "users.view", "users.create", "users.edit", "users.delete",
    "roles.view", "roles.create", "roles.edit", "roles.delete",
    "org.view", "org.edit", "org.settings",
    "security.view", "security.manage",
    "audit.view", "audit.export",
    "settings.view", "settings.edit",
    "integrations.view", "integrations.configure",
    "workflows.view", "workflows.create", "workflows.edit",
    "admin.full_access", "admin.system_config",
    # ... 40+ permissions total
]

# Update Super Admin role in database
super_admin_role.permissions = json.dumps(ALL_PERMISSIONS)
session.commit()
```

**Files Changed:**
- `scripts/fix_super_admin_permissions.py` (NEW): Script to populate Super Admin permissions
- `Dockerfile` (line 88-90): Added script to startup sequence

#### Verification:
```bash
# After fix, check permissions in database:
python -c "
from database.base import get_db_session
from database.models.role import Role
import json

with get_db_session() as session:
    role = session.query(Role).filter_by(name='Super Admin').first()
    perms = json.loads(role.permissions)
    print(f'Permissions: {len(perms)} total')
    print(perms[:5])  # Show first 5
"
# Output:
# Permissions: 40 total
# ['agents.view', 'agents.create', 'agents.edit', 'agents.delete', 'agents.publish']
```

#### Prevention:
1. ‚úÖ **Validate Role Data After Migration:**
   ```python
   # Add to migration script:
   def validate_role_permissions(session):
       critical_roles = ['Super Admin', 'Admin']
       for role_name in critical_roles:
           role = session.query(Role).filter_by(name=role_name).first()
           perms = json.loads(role.permissions) if role.permissions else []
           if not perms:
               print(f"‚ùå CRITICAL: {role_name} has no permissions!")
               return False
       return True
   ```

2. ‚úÖ **Seed Default Permissions for System Roles:**
   ```python
   # In database/init_db.py or migration script:
   DEFAULT_PERMISSIONS = {
       'Super Admin': ALL_PERMISSIONS,  # Full access
       'Admin': ADMIN_PERMISSIONS,      # Most access
       'User': USER_PERMISSIONS,        # Basic access
   }
   
   for role_name, perms in DEFAULT_PERMISSIONS.items():
       role = session.query(Role).filter_by(name=role_name).first()
       if role and not role.permissions:
           role.permissions = json.dumps(perms)
   ```

3. ‚úÖ **Add Permission Validation to Pre-Commit:**
   ```bash
   # In scripts/comprehensive_db_check.sh:
   echo "Checking role permissions..."
   python -c "
   from database.services.role_service import RoleService
   roles = RoleService.get_all_roles()
   system_roles = [r for r in roles if r.is_system]
   for role in system_roles:
       if not role.permissions:
           print(f'‚ùå {role.name} has no permissions!')
           exit(1)
   "
   ```

4. ‚úÖ **Document Permission Management:**
   - Create `docs/PERMISSIONS.md` listing all available permissions
   - Document which roles should have which permissions
   - Add permission audit script to check role configurations

5. ‚úÖ **Frontend Fallback:**
   ```javascript
   // In frontend, add fallback for Super Admin:
   if (user.role_ids.includes(SUPER_ADMIN_ROLE_ID) && user.permissions.length === 0) {
       console.warn('Super Admin with empty permissions - showing all menu items');
       // Show all menu items as fallback
   }
   ```

#### Related Issues:
- Issue #21: Role permissions not stored in migration (parent issue)
- Issue #26: Double JSON encoding in role_ids (unrelated but discovered while debugging this)
- RECURRING PATTERN #1: Incomplete data validation across migration pipeline

#### Key Takeaway:
**Schema correctness ‚â† Data validity!**

Even when:
- ‚úÖ Schema is correct (all fields present)
- ‚úÖ Migration logic is correct (all fields mapped)
- ‚úÖ No validation errors (Pydantic passes)

You can STILL have **empty or invalid data** if:
- ‚ùå Source data is incomplete
- ‚ùå No default values for required fields
- ‚ùå No validation of business logic constraints

**Always validate critical data, not just schema!**

---

### Issue #28: Multiple Duplicate Super Admin Roles (Only First One Updated)
**Date:** 2026-01-13
**Severity:** CRITICAL
**Occurrences:** 1 time (scripts/fix_super_admin_permissions.py)
**Related to:** RECURRING PATTERN #1 (Incomplete Data Migration / Query Logic)

#### Problem:
After successfully fixing Issue #27 (empty permissions in Super Admin role), the UI menu still didn't appear. Investigation revealed:

1. **User had role_id:** `"911de94c-9df7-4ec4-9313-ad759acf9e78"`
2. **Script updated role_id:** `"4e7a15db-753f-4f9f-ae1a-8ad34a7d9790"`
3. **User's `/api/security/auth/me` response:**
   ```json
   {
     "role_ids": ["911de94c-9df7-4ec4-9313-ad759acf9e78"],
     "permissions": []  <-- STILL EMPTY!
   }
   ```

#### Root Cause:
The `fix_super_admin_permissions.py` script used `.first()` to find and update ONE Super Admin role:
```python
super_admin_role = session.query(Role).filter_by(name="Super Admin").first()
```

However, the database contained **multiple Super Admin roles** with different UUIDs (created during migration due to previous bugs). The script only updated the first role it found, leaving other Super Admin roles with empty permissions.

#### Error Pattern:
```bash
# From Railway logs:
‚úÖ Found Super Admin role: 4e7a15db-753f-4f9f-ae1a-8ad34a7d9790  # Updated this one
‚úÖ Permissions updated. Added 87 new permissions.

# But user had a DIFFERENT Super Admin role:
User role_ids: ["911de94c-9df7-4ec4-9313-ad759acf9e78"]  # Not updated!
User permissions: []  # Still empty!
```

#### Solution:
Changed the script to update **ALL** Super Admin roles, not just the first one:

```python
# OLD (WRONG):
super_admin_role = session.query(Role).filter_by(name="Super Admin").first()
super_admin_role.permissions = json.dumps(ALL_PERMISSIONS)

# NEW (CORRECT):
super_admin_roles = session.query(Role).filter_by(name="Super Admin").all()  # Get ALL
for role in super_admin_roles:
    role.permissions = json.dumps(ALL_PERMISSIONS)
```

**File Modified:** `scripts/fix_super_admin_permissions.py`

#### Why It Happened:
1. **Migration Script Created Duplicates:** Previous migration bugs created multiple Super Admin roles with different UUIDs (visible in deployment logs with many "Using existing UUID for 'Super Admin'..." messages)
2. **Assumption of Uniqueness:** Script assumed there was only ONE Super Admin role
3. **No Duplicate Detection:** No validation to check for or clean up duplicate roles
4. **No Role Consolidation:** Old duplicate roles were never cleaned up

#### Prevention:
1. ‚úÖ **Always Use `.all()` for System Roles:**
   ```python
   # When updating system roles that might have duplicates:
   roles = session.query(Role).filter_by(name="Super Admin").all()
   if len(roles) > 1:
       print(f"‚ö†Ô∏è WARNING: Found {len(roles)} roles named 'Super Admin'")
   for role in roles:
       # Update ALL instances
       role.permissions = json.dumps(ALL_PERMISSIONS)
   ```

2. ‚úÖ **Add Duplicate Detection Script:**
   ```python
   # scripts/detect_duplicate_roles.py
   def find_duplicate_roles():
       with get_db_session() as session:
           roles = session.query(Role.name, func.count(Role.id)).group_by(Role.name).having(func.count(Role.id) > 1).all()
           for role_name, count in roles:
               print(f"‚ö†Ô∏è Duplicate role '{role_name}': {count} instances")
               instances = session.query(Role).filter_by(name=role_name).all()
               for instance in instances:
                   print(f"   - ID: {instance.id}, Permissions: {len(json.loads(instance.permissions or '[]'))}")
   ```

3. ‚úÖ **Add Unique Constraint to Role Name (Per Org):**
   ```python
   # In database/models/role.py:
   class Role(Base):
       __tablename__ = "roles"
       __table_args__ = (
           UniqueConstraint('name', 'org_id', name='uq_role_name_org'),
       )
   ```

4. ‚úÖ **Migration Script Cleanup Phase:**
   ```python
   # After migration, consolidate duplicates:
   def consolidate_duplicate_roles(session):
       duplicates = session.query(Role.name, Role.org_id).group_by(Role.name, Role.org_id).having(func.count(Role.id) > 1).all()
       for name, org_id in duplicates:
           roles = session.query(Role).filter_by(name=name, org_id=org_id).order_by(Role.created_at).all()
           primary = roles[0]  # Keep the oldest
           duplicates = roles[1:]
           
           # Update users to use primary role
           for dup in duplicates:
               users = session.query(User).filter(User.role_ids.contains(str(dup.id))).all()
               for user in users:
                   role_ids = json.loads(user.role_ids)
                   role_ids = [str(primary.id) if r == str(dup.id) else r for r in role_ids]
                   user.role_ids = json.dumps(role_ids)
               session.delete(dup)
   ```

5. ‚úÖ **Log All Role IDs During Updates:**
   ```python
   print(f"‚úÖ Found {len(super_admin_roles)} Super Admin role(s):")
   for i, role in enumerate(super_admin_roles, 1):
       print(f"   {i}. UUID: {role.id}, Permissions: {len(role.permissions or [])}")
   ```

#### Related Issues:
- Issue #19: Missing role_ids in User Migration (created duplicate role scenarios)
- Issue #21: Missing Role Permissions in Migration (created empty permission scenarios)
- Issue #27: Empty Permissions in Super Admin Role (parent issue - this is the follow-up)
- RECURRING PATTERN #1: Incomplete data queries/updates across entities

#### Key Takeaway:
**When working with system/critical entities:**
1. Always assume duplicates may exist (especially after migrations)
2. Use `.all()` instead of `.first()` for updates
3. Log all affected entities
4. Add duplicate detection to validation scripts
5. Clean up duplicates proactively

**Script Assumptions ‚â† Database Reality!**

Even when:
- ‚úÖ Schema enforces uniqueness (or should)
- ‚úÖ Business logic prevents duplicates (in theory)
- ‚úÖ You expect only one record

You can STILL have **duplicates** if:
- ‚ùå Migration scripts ran multiple times
- ‚ùå Unique constraints were added after data was inserted
- ‚ùå Previous bugs created duplicates

**Always query for ALL instances, not just the first!**

---

**üí° Remember: Prevention is better than debugging on production!**
**üö® ZERO TOLERANCE for repeated mistakes - system will block them!**
